---
title: "Tree Based Methods"
author: "Ahlam Abuawad"
date: "7/10/2020"
output: html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
#install.packages("tidyverse")
library(tidyverse)
#install.packages("janitor")
library(janitor)
#install.packages("caret")
library(caret)
#install.packages("Hmisc")
library(Hmisc)
#install.packages("pROC")
library(pROC)
#install.packages("rpart")
library(rpart)
#install.packages("rpart.plot")
library(rpart.plot)
#install.packages("tree")
library(tree)
#install.packages("ranger")
library(ranger)
#install.packages("randomForest")
library(randomForest)
#install.packages("gbm")
library(gbm)
#install.packages("knitr")
library(knitr)
```

# Data Import and Cleaning 

First, load the dataset; clean up names as needed; and convert factors to, well, factors. 

```{r}
study_pop = read_csv(here::here("Data/studypop.csv")) %>% 
  clean_names(case = c("old_janitor")) %>% 
  mutate(bmi_cat3 = as.factor(bmi_cat3),
         edu_cat = as.factor(edu_cat),
         race_cat = as.factor(race_cat),
         male = as.factor(male)) 
```

Quick data descriptions; because of the length of the output, we don't execute this command here, but encourage you to!

```{r, eval = FALSE}
#describe(study_pop)
```

Next we remove missing values and reorder predictors (environmental variables first, confounders second). In keeping with standard practice, we'll ln-transform the environmental exposures and the outcome. This is the dataset we'll use to illustrate variable selection methods. 

```{r}
data_tree = study_pop %>% 
  mutate_at(vars(contains("la")), log) %>% 
  mutate(log_telomean = log(telomean)) %>% 
  dplyr::select(log_telomean, lbx074la:lbx187la, lbxd03la:lbx194la, everything(), -seqn, -telomean) %>% 
  na.omit(log_telomean) 

names(data_tree)

dim(data_tree)
```

Trees utilize a training set to model set parameters that can be applied to a test set (remaining observations). 

```{r}
set.seed(1993) # for every cv step

# creating training set
train_telo <- sample(1:nrow(data_tree), floor(nrow(data_tree)/2))
data_train <- data_tree[train_telo,]

# creating test set
data_test <- data_tree[-train_telo,]
```

Let's take a quick look at our two datasets.

```{r}
dim(data_train) # 501 observations
dim(data_test) # 502 observations

#View(data_train)
```

# Classification Trees

We'll start by fitting a classification tree to the training data, with log telomere length as the response and the other variables as predictors. We'll use cross-validation (CV) on the training set in order to determine the optimal tree size.

```{r}
### Fitting a single tree using Recursive Partitioning and Regression Trees (rpart)
fit <- rpart(formula = log_telomean ~ ., 
             data = data_train,
             control = rpart.control(cp = 0.005))

# cp - Complexity Parameter; Any split that does not decrease the overall lack of fit by a factor of cp is not attempted. Essentially,the user decides that any split that doesn't improve the fit by cp will likely be pruned off by CV.

# Display the Complexity Parameter table for fitted rpart
cpTab <- printcp(fit)
knitr::kable(cpTab)

# Visual representation of CV results from rpart
plotcp(fit, col = "red") # red horizontal line is drawn 1 SE above the minimum of the curve
```

We can look at the tree with minimum CV error (and the best tree using the 1 SE rule).

```{r}
# Determining the size of the tree with min CV error
minErr <- which.min(cpTab[, 4]) # 3

# Creating a tree based on the minErr
tree_best <- prune(fit, cp = cpTab[minErr, 1])
tree_best

# Checking for trees with minimum complexity
good_telo <- which(cpTab[, 4] < cpTab[minErr, 4] + cpTab[minErr, 5]) # 2, 3, 4
min_complexity_telo <- good_telo[1] # 2

# Creating tree based on 1 SE rule
tree_1se <- prune(fit, cp = cpTab[min_complexity_telo, 1])
```

We can explore the results using a plot of the tree.

```{r}
# Tree based on 1 SE rule
rpart.plot(tree_1se)

# Tree based on minimum CV error
rpart.plot(tree_best)
```

We can measure "how important" each variable is for the final result, and calculate the mean squared error (MSE) of the tree result. 

```{r}
# Variable importance - Represents the decrease in the variance/impurity caused by the addition of a variable to the tree.
tree_1se$variable.importance/max(tree_1se$variable.importance)
tree_best$variable.importance/max(tree_best$variable.importance)

# Predict values onto test set
preds_1se <- predict(tree_1se, data_test)
preds_best <- predict(tree_best, data_test)

# Average log telomean length
mean(data_test$log_telomean)

# Calculate the mean squared error (MSE)
(MSE_1tree_1se <- mean((preds_1se - data_test$log_telomean)^2))
(MSE_1tree_1best <- mean((preds_best - data_test$log_telomean)^2))
```


# Bagging

We'll perform bagging on the training set, and then use the importance() function to determine which variables are most important. 

```{r bag}
set.seed(1993)
bag_telo = randomForest(log_telomean ~ ., 
                        data = data_train,
                        mtry = 17, 
                        importance = TRUE) 

# mtry should equal number of variables, it's the # of variables randomly sampled as candidates at each split. 

# View variable importance based on model
importance(bag_telo)
varImpPlot(bag_telo)

# add similar plot with only chemicals, not confounders

# Predict values onto test set
preds_bag = predict(bag_telo, newdata = data_test, type = "class")

# Calculate the MSE
(MSE_bag <- mean((preds_bag - data_test$log_telomean)^2))
```


# Random Forests

In practice, aggregating trees is more useful. We try using a random forest with $5$ variables per split.

```{r}
# Fitting a random forest using randomForest function
fit_rf <- randomForest(log_telomean ~ ., 
                       data = data_train,
                       mtry = 10)

# View variable importance based on model
importance(fit_rf)
varImpPlot(fit_rf)

# Fitting a random forest using ranger function
fit_ranger <- ranger(log_telomean ~ ., 
                       data = data_train,
                     min.node.size = 20,
                     num.trees = 1000)
fit_ranger

# Predict values onto test set
preds_rf <- predict(fit_rf, data_test)
preds_ranger <- predict(fit_ranger, data_test)$predictions

# View observed vs predicted values from ranger random forest
data_test %>% 
  ggplot(aes(x = preds_ranger, y = log_telomean)) +
  geom_point(colour = "#ff6767", alpha = 0.3) +
  labs(title = "Predicted vs Observed", 
       x = "Predicted Telomere Length", 
       y = "Oberved Telomere Length") +  theme_bw(18)

# Calculate MSE
(MSE_rf <- mean((preds_rf - data_test$log_telomean)^2))
(MSE_ranger <- mean((preds_ranger - data_test$log_telomean)^2))
```


# Comparison of Tree-Based Models

Compare the error rate of different tree-based models and discuss your results.

```{r compare}
test_MSE = cbind(MSE_1tree_1se, MSE_1tree_1best, MSE_bag, MSE_rf, MSE_ranger)

rownames(test_MSE) = colnames(MSE_1tree_1se); colnames(test_MSE) = c("Tree 1 SE", "Tree Best", "Bagging", "Random Forest", "Ranger")

knitr::kable(test_MSE, align = "c")
```

Traditional classical tree-based methods are not able to control for confounders outside the tree part (i.e. the final tree usually consists of confounders and explanatory variables, making it difficult to compare to other methods). 

Additionally, the lack of coefficients/exposure response curves makes it difficult to interpret the results. However, we will explore this further with Bayesian Additive Regression Trees (BART).

