---
title: "Tree Based Methods -- BART"
author: "Lizzy Gibson"
date: "7/21/2020"
output: 
  html_document:
    toc: TRUE
    toc_float: TRUE
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
#install.packages("tidyverse")
library(tidyverse)
#install.packages("janitor")
library(janitor)
#install.packages("caret")
library(caret)
#install.packages("knitr")
library(knitr)
#install.packages("BART")
library(BART)
```

# Data Import and Cleaning 

First, load the dataset; clean up names as needed; and convert factors to, well, factors. 

```{r}
study_pop <- read_csv(here::here("Data/studypop.csv")) %>% 
  clean_names(case = c("old_janitor")) %>% 
  mutate(bmi_cat3 = as.factor(bmi_cat3),
         edu_cat = as.factor(edu_cat),
         race_cat = as.factor(race_cat),
         male = as.factor(male)) 
```

Next we remove missing values and reorder predictors (environmental variables first, confounders second). In keeping with standard practice, we'll ln-transform the environmental exposures and the outcome. This is the dataset we'll use to illustrate variable selection methods. 

```{r}
data_tree <- study_pop %>% 
  mutate_at(vars(contains("la")), log) %>% # log POP
  mutate_at(vars(contains("lbx")), scale) %>% # scale logged POP and scale white blood cells
  mutate_at(vars(contains("age")), scale) %>% # scale age and age^2
  mutate(scale_log_telomean = scale(log(telomean))) %>% 
  dplyr::select(scale_log_telomean, lbx074la:lbx187la, lbxd03la:lbx194la, everything(), -seqn, -telomean) %>% 
  na.omit(log_telomean) 
```

Trees utilize a training set to model set parameters that can be applied to a test set (remaining observations). BART requires the outcome variable (y.train) to be a vector of numeric values. The covariates for training (validation, if any) are x.train (x.test) which can be matrices or dataframes.

```{r}
data_expanded <- model.matrix(scale_log_telomean ~ ., data_tree)[,-1]

set.seed(1993)
train_sample <- sample(1:nrow(data_tree), floor(nrow(data_tree)/2))

# Create a matrix of predictors as x
x_train <- data_expanded[train_sample,]
x_test <- data_expanded[-train_sample,]

# Extract outcome vector
y_train <- as.vector(as.matrix(data_tree[train_sample,1]))
y_test <- as.vector(as.matrix(data_tree[-train_sample,1]))
```

## Bayesian Additive Regression Trees

See the vignette [here.](https://cran.r-project.org/web/packages/BART/vignettes/the-BART-R-package.pdf)

Fit a BART model to the training data, with log telomere length as the response and the other variables as predictors.

```{r, run, cache=TRUE}
set.seed(1988)
post <- wbart(x_train, y_train, nskip=1000, ndpost=1000, keepevery=100) 
# nskip = burnin
# ndpost = number of MCMC samples returned
# keepevery = thinning
# wbart is for continuous outcome variables -- see gbart for binary.
```

Post is a list.

```{r}
names(post)

dim(post$yhat.train)
# 1000 draws for each participant
length(post$yhat.train.mean)
# average predicted LTL over all draws
length(post$sigma)
# number of post burn-in draws, counting thinned
```

## Assess convergence
```{r}
plot(post$sigma, type="l")
abline(v=1000, col="red") # separates burn-in
```

## Predict

Order the observations by the fitted value (yhat.train.mean), and use boxplots to display the draws of f(x) in each column of yhat.train.

```{r}
i <- order(post$yhat.train.mean)
boxplot(post$yhat.train[, i])
```

Substantial predictive uncertainty, but you can still see a range of logLTL values.

Predicted outcomes for testing set.

```{r}
pred <- predict(post, x_test)
dim(pred)
# 1000 draws for each participant

#take the average
yhat <- apply(pred, 2, mean)
length(yhat)
```

Mean squared error on test set.

```{r}
mean((yhat - y_test)^2)
```

Correlation.

```{r}
cor(cbind(y_test, yhat))
```

### Viz

```{r}
cbind(y_test, yhat) %>% as_tibble() %>% 
  ggplot(aes(x = y_test, y = yhat)) +
  geom_point() +
  geom_abline(slope=1,intercept=0,color="pink", linetype="dashed") +
  theme_bw()
```

## Calculate marginal effects

Friedmanâ€™s partial dependence function (Friedman 2001) can be employed with BART to summarize the marginal effect due to a subset of the covariates. Care must be taken in the interpretation of the marginal effect. If there are strong relationships among the covariates, it may be unrealistic to assume that individual covariates can be manipulated independently.

Suppose that we want to summarize log LTL by a single POP while aggregating over the other covariates. Here, we demonstrate how to calculate a marginal estimate and its 95% credible interval.

```{r}
colnames(x_train)[13] # predictor of interest
dim(x_train)
x_marg <- cbind(x_train[,-13], x_train[,13])
dim(x_marg)

set.seed(1988)
post_marg = wbart(x_marg, y_train) # same y vector, x in new order
H = floor(0.75*length(y_train))
L = 10

x_seq = seq(min(x_marg[, 36]), max(x_marg[, 36]), length.out=L) 
# creates vector from min to max of length L
x_test_marg = cbind(x_marg[, -36], x_seq[1])
# same predictors except predictor of interest set to min

for (j in 2:L) {
  x_test_marg = rbind(x_test_marg, cbind(x_marg[, -36], x_seq[j]))
  }
# add training data with predictor of interest set to vector x values
# kind of like exposed v unexposed

pred_marg = predict(post_marg, x_test_marg)
# predict on x.test + x.train with predictor of interest fixed in test and varied in train

partial = matrix(nrow=1000, ncol=L)
for (j in 1:L) {
    h = (j - 1) * H + 1:H
    partial[, j] = apply(pred_marg[, h], 1, mean)
}

plot(x_seq, apply(partial, 2, mean), type='l',
     xlab='Furan lbxf04la', ylab='log LTL',ylim=c(-0.1, 0.1))
lines(x_seq, apply(partial, 2, quantile, probs=0.025), lty=2)
lines(x_seq, apply(partial, 2, quantile, probs=0.975), lty=2)
abline(h=0, col="red",lty=3)
```

